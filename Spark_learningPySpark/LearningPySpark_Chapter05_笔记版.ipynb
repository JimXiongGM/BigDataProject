{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing MLib package of PySpark \n",
    "\n",
    "## 第5章 MLlib介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the previous chapter, we first specify the schema of our dataset.\n",
    "\n",
    "```bash\n",
    "wget http://www.tomdrabas.com/data/LearningPySpark/births_train.csv.gz;\n",
    "cd /root/jupyternotebook/learningPySpark-master/Chapter05/;\n",
    "hadoop fs -put ./births_train.csv.gz /jupyter-notebook-data/;\n",
    "```\n",
    "MLlib代表机器学习库。即使MLlib现在处于维护模式，即它没有被积极开发（并且很可能在以后被弃用），但我们保证至少会覆盖该库的一些特性。此外，MLlib是目前唯一支持流媒体训练模型的库。\n",
    "\n",
    "从Spark 2.0开始，ML是主要的机器学习库，它对DataFrame进行操作，而不像MLlib那样对RDD进行操作。\n",
    "\n",
    "MLlib概括了其公开三个核心机器学习功能：\n",
    "- 数据准备：特征提取、变换、选择、分类特征的散列和一些自然语言处理方法。\n",
    "- 机器学习算法：实现了一些流行和高级的回归，分类和聚类算法。\n",
    "- 实用程序：统计方法，如描述性统计、卡方检验、线性代数（稀疏稠密矩阵和向量）和模型评估方法。\n",
    "\n",
    "如您所见，可用功能面板可以让您执行几乎所有的基础数据科学任务。\n",
    "\n",
    "在本章中，我们将构建两个分类模型：线性回归和随机森林。我们将使用从http://www.cdc.gov/nchs/data_access/vitalstatsonline.htm 下载的美国2014年和2015年出生数据的一部分。我们从300个变量中选择了85个特征，我们将使用它们来构建我们的模型。此外，在总数近799万条记录中，我们选择了45429条记录的平衡样本：22080条婴儿死亡的记录，23349条婴儿活着的记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ\n",
    "\n",
    "labels = [\n",
    "    ('INFANT_ALIVE_AT_REPORT', typ.StringType()),\n",
    "    ('BIRTH_YEAR', typ.IntegerType()),\n",
    "    ('BIRTH_MONTH', typ.IntegerType()),\n",
    "    ('BIRTH_PLACE', typ.StringType()),\n",
    "    ('MOTHER_AGE_YEARS', typ.IntegerType()),\n",
    "    ('MOTHER_RACE_6CODE', typ.StringType()),\n",
    "    ('MOTHER_EDUCATION', typ.StringType()),\n",
    "    ('FATHER_COMBINED_AGE', typ.IntegerType()),\n",
    "    ('FATHER_EDUCATION', typ.StringType()),\n",
    "    ('MONTH_PRECARE_RECODE', typ.StringType()),\n",
    "    ('CIG_BEFORE', typ.IntegerType()),\n",
    "    ('CIG_1_TRI', typ.IntegerType()),\n",
    "    ('CIG_2_TRI', typ.IntegerType()),\n",
    "    ('CIG_3_TRI', typ.IntegerType()),\n",
    "    ('MOTHER_HEIGHT_IN', typ.IntegerType()),\n",
    "    ('MOTHER_BMI_RECODE', typ.IntegerType()),\n",
    "    ('MOTHER_PRE_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),\n",
    "    ('DIABETES_PRE', typ.StringType()),\n",
    "    ('DIABETES_GEST', typ.StringType()),\n",
    "    ('HYP_TENS_PRE', typ.StringType()),\n",
    "    ('HYP_TENS_GEST', typ.StringType()),\n",
    "    ('PREV_BIRTH_PRETERM', typ.StringType()),\n",
    "    ('NO_RISK', typ.StringType()),\n",
    "    ('NO_INFECTIONS_REPORTED', typ.StringType()),\n",
    "    ('LABOR_IND', typ.StringType()),\n",
    "    ('LABOR_AUGM', typ.StringType()),\n",
    "    ('STEROIDS', typ.StringType()),\n",
    "    ('ANTIBIOTICS', typ.StringType()),\n",
    "    ('ANESTHESIA', typ.StringType()),\n",
    "    ('DELIV_METHOD_RECODE_COMB', typ.StringType()),\n",
    "    ('ATTENDANT_BIRTH', typ.StringType()),\n",
    "    ('APGAR_5', typ.IntegerType()),\n",
    "    ('APGAR_5_RECODE', typ.StringType()),\n",
    "    ('APGAR_10', typ.IntegerType()),\n",
    "    ('APGAR_10_RECODE', typ.StringType()),\n",
    "    ('INFANT_SEX', typ.StringType()),\n",
    "    ('OBSTETRIC_GESTATION_WEEKS', typ.IntegerType()),\n",
    "    ('INFANT_WEIGHT_GRAMS', typ.IntegerType()),\n",
    "    ('INFANT_ASSIST_VENTI', typ.StringType()),\n",
    "    ('INFANT_ASSIST_VENTI_6HRS', typ.StringType()),\n",
    "    ('INFANT_NICU_ADMISSION', typ.StringType()),\n",
    "    ('INFANT_SURFACANT', typ.StringType()),\n",
    "    ('INFANT_ANTIBIOTICS', typ.StringType()),\n",
    "    ('INFANT_SEIZURES', typ.StringType()),\n",
    "    ('INFANT_NO_ABNORMALITIES', typ.StringType()),\n",
    "    ('INFANT_ANCEPHALY', typ.StringType()),\n",
    "    ('INFANT_MENINGOMYELOCELE', typ.StringType()),\n",
    "    ('INFANT_LIMB_REDUCTION', typ.StringType()),\n",
    "    ('INFANT_DOWN_SYNDROME', typ.StringType()),\n",
    "    ('INFANT_SUSPECTED_CHROMOSOMAL_DISORDER', typ.StringType()),\n",
    "    ('INFANT_NO_CONGENITAL_ANOMALIES_CHECKED', typ.StringType()),\n",
    "    ('INFANT_BREASTFED', typ.StringType())\n",
    "]\n",
    "\n",
    "schema = typ.StructType([\n",
    "        typ.StructField(e[0], e[1], False) for e in labels\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the data.\n",
    "\n",
    "虽然MLlib是着重为RDD和DStream设计的，但是为了方便转换数据，我们将读取数据并将其转换为DataFrame。\n",
    "\n",
    "接下来，我们来加载数据。.read.csv（...）方法可以读取未压缩的或（如同本例中）“GZipped”压缩的逗号分隔的值。参数header设置为true，代表第一行包含头，而且我们使用schema来指定正确的数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "#---------------------------#\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder \\\n",
    "    .master('spark://master:7077') \\\n",
    "    .appName('Introducing MLib package of PySpark') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "births = spark.read.csv('/jupyter-notebook-data/births_train.csv.gz', \n",
    "                        header=True, \n",
    "                        schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify our recode dictionary.\n",
    "\n",
    "首先定义重编码字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recode_dictionary = {\n",
    "    'YNU': {\n",
    "        'Y': 1,\n",
    "        'N': 0,\n",
    "        'U': 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict whether the `'INFANT_ALIVE_AT_REPORT'` is either 1 or 0. Thus, we will drop all of the features that relate to the infant.\n",
    "\n",
    "我们本章的目标是预测“INFANT_ALIVE_AT_REPORT”是1还是0。因此，我们将丢弃与婴儿相关的所有特征，而仅仅基于与其母亲、父亲和出生地点相关的特征来预测婴儿的存活机会："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'INFANT_ALIVE_AT_REPORT', \n",
    "    'BIRTH_PLACE', \n",
    "    'MOTHER_AGE_YEARS', \n",
    "    'FATHER_COMBINED_AGE', \n",
    "    'CIG_BEFORE', \n",
    "    'CIG_1_TRI', \n",
    "    'CIG_2_TRI', \n",
    "    'CIG_3_TRI', \n",
    "    'MOTHER_HEIGHT_IN', \n",
    "    'MOTHER_PRE_WEIGHT', \n",
    "    'MOTHER_DELIVERY_WEIGHT', \n",
    "    'MOTHER_WEIGHT_GAIN', \n",
    "    'DIABETES_PRE', \n",
    "    'DIABETES_GEST', \n",
    "    'HYP_TENS_PRE', \n",
    "    'HYP_TENS_GEST', \n",
    "    'PREV_BIRTH_PRETERM'\n",
    "]\n",
    "\n",
    "births_trimmed = births.select(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the recoding methods.\n",
    "\n",
    "在我们的数据集中有大量的特征，它们的值是Yes/No/Unknown；我们将仅仅把Yes编码为1，其他值设置为0。\n",
    "\n",
    "还有个小问题是，母亲吸烟的数量如何编码：因为0意味着母亲在怀孕前或怀孕期间没有吸烟；1～97之间代表的是母亲实际吸烟数量；98代表母亲实际吸烟数量是98或更多；而99代表母亲实际吸烟数量未知，我们将假设未知状态为0，并以此重新编码。\n",
    "\n",
    "接下来我们将指定我们的重新编码方法：\n",
    "\n",
    "recode 方法从 recode_dictionary（给出键）查找正确的键，并返回更正的值。\n",
    "\n",
    "correct_cig 方法检查如下，当 feat 特征值不等于 99 时，返回特征值；如果值等于 99，我们则得到 0。\n",
    "\n",
    "我们不能直接在 DataFrame 上使用 recode 函数，它需要转换为 Spark 可理解的 UDF（用户定义函数）。rec_integer 函数功能如下：通过传递我们指定的 recode **函数**及指定**返回值**数据类型，我们可以用它来重新编码 Yes/No/Unknown 特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "\n",
    "#recode_dictionary = {\n",
    "#    'YNU': {\n",
    "#        'Y': 1,\n",
    "#        'N': 0,\n",
    "#        'U': 0\n",
    "#    }\n",
    "#}\n",
    "\n",
    "def recode(col, key):        \n",
    "    return recode_dictionary[key][col] \n",
    "\n",
    "def correct_cig(feat):\n",
    "    return func \\\n",
    "        .when(func.col(feat) != 99, func.col(feat))\\\n",
    "        .otherwise(0)\n",
    "rec_integer = func.udf(recode, typ.IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct the features related to the number of smoked cigarettes.\n",
    "\n",
    ".withColumn（...）方法用列名作为其第一个参数，用转换作为第二个参数。在以前的例子中，我们不会创建新列，而是重用相同的列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "births_transformed = births_trimmed \\\n",
    "    .withColumn('CIG_BEFORE', correct_cig('CIG_BEFORE'))\\\n",
    "    .withColumn('CIG_1_TRI', correct_cig('CIG_1_TRI'))\\\n",
    "    .withColumn('CIG_2_TRI', correct_cig('CIG_2_TRI'))\\\n",
    "    .withColumn('CIG_3_TRI', correct_cig('CIG_3_TRI'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out which Yes/No/Unknown features are.\n",
    "\n",
    "现在我们将集中更正 Yes/No/Unknown 特征。首先，用下面的代码段来确定有哪些：\n",
    "\n",
    "首先，我们创建了一个包含列名称和相应数据类型的元组（cols）列表。接下来，我们循环遍历这些列表，并计算所有字符串列的不同值；如果“Y”在返回的列表中，我们将列名追加到YNU_cols列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFANT_ALIVE_AT_REPORT\n",
      "DIABETES_PRE\n",
      "DIABETES_GEST\n",
      "HYP_TENS_PRE\n",
      "HYP_TENS_GEST\n",
      "PREV_BIRTH_PRETERM\n"
     ]
    }
   ],
   "source": [
    "cols = [(col.name, col.dataType) for col in births_trimmed.schema]\n",
    "\n",
    "YNU_cols = []\n",
    "\n",
    "for i, s in enumerate(cols):\n",
    "    if s[1] == typ.StringType():\n",
    "        dis = births.select(s[0]) \\\n",
    "            .distinct() \\\n",
    "            .rdd \\\n",
    "            .map(lambda row: row[0]) \\\n",
    "            .collect()\n",
    "\n",
    "        if 'Y' in dis:\n",
    "            YNU_cols.append(s[0])\n",
    "            print (s[0])\n",
    "# 其实这里说的是把每一列取值弄成一个set，看看Y在不在其中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "('INFANT_ALIVE_AT_REPORT', StringType)\n",
      "<class 'int'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "cols = [(col.name, col.dataType) for col in births_trimmed.schema]\n",
    "for i, s in enumerate(cols):\n",
    "    print (i)\n",
    "    print (s)\n",
    "    print (type(i))\n",
    "    print (type(s))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y', 'N']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "dis_test = births.select('INFANT_ALIVE_AT_REPORT').distinct()\\\n",
    ".rdd.map(lambda row: row[0]).collect()\n",
    "\n",
    "print (dis_test)\n",
    "print (type(dis_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[INFANT_ALIVE_AT_REPORT: string, BIRTH_PLACE: string, MOTHER_AGE_YEARS: int, FATHER_COMBINED_AGE: int, CIG_BEFORE: int, CIG_1_TRI: int, CIG_2_TRI: int, CIG_3_TRI: int, MOTHER_HEIGHT_IN: int, MOTHER_PRE_WEIGHT: int, MOTHER_DELIVERY_WEIGHT: int, MOTHER_WEIGHT_GAIN: int, DIABETES_PRE: string, DIABETES_GEST: string, HYP_TENS_PRE: string, HYP_TENS_GEST: string, PREV_BIRTH_PRETERM: string] \n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+----------------------+-----------+----------------+-------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+\n",
      "|INFANT_ALIVE_AT_REPORT|BIRTH_PLACE|MOTHER_AGE_YEARS|FATHER_COMBINED_AGE|CIG_BEFORE|CIG_1_TRI|CIG_2_TRI|CIG_3_TRI|MOTHER_HEIGHT_IN|MOTHER_PRE_WEIGHT|MOTHER_DELIVERY_WEIGHT|MOTHER_WEIGHT_GAIN|DIABETES_PRE|DIABETES_GEST|HYP_TENS_PRE|HYP_TENS_GEST|PREV_BIRTH_PRETERM|\n",
      "+----------------------+-----------+----------------+-------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+\n",
      "|                     N|          1|              29|                 99|        99|       99|       99|       99|              99|              999|                   999|                99|           N|            N|           N|            N|                 N|\n",
      "+----------------------+-----------+----------------+-------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (births_trimmed,'\\n')\n",
    "print (type(births_trimmed))\n",
    "births_trimmed.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames can transform the features *in bulk* while selecting features.\n",
    "\n",
    "我们选择“INFANT_NICU_ADMISSION”列，并且将该特征的名称传递给rec_integer方法。我们还将新转换的列的别名称为“INFANT_NICU_ADMISSION_RECODE”。这样我们还可确保UDF按预期工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 上文的代码\n",
    "\n",
    "```py\n",
    "recode_dictionary = {\n",
    "    'YNU': {\n",
    "        'Y': 1,\n",
    "        'N': 0,\n",
    "        'U': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "def recode(col, key):        \n",
    "    return recode_dictionary[key][col] \n",
    "\n",
    "def correct_cig(feat):\n",
    "    return func \\\n",
    "        .when(func.col(feat) != 99, func.col(feat))\\\n",
    "        .otherwise(0)\n",
    "rec_integer = func.udf(recode, typ.IntegerType())\n",
    "```\n",
    "**为什么**rec_integer能够这样使用？？\n",
    "\n",
    "推测：注册为udf后，所有参数都是整列！这里的调用方法是\n",
    "```python3\n",
    "rec_integer('INFANT_NICU_ADMISSION', func.lit('YNU'))\n",
    "```\n",
    "说明第一个参数是选定的一整列，第二个参数是所有cell的值都为`YNU`的一列！然后函数读取YNU对应的0或1，并根据第一列相应cell的value进行判断并返回值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_NICU_ADMISSION='Y', INFANT_NICU_ADMISSION_RECODE=1),\n",
       " Row(INFANT_NICU_ADMISSION='Y', INFANT_NICU_ADMISSION_RECODE=1),\n",
       " Row(INFANT_NICU_ADMISSION='U', INFANT_NICU_ADMISSION_RECODE=0),\n",
       " Row(INFANT_NICU_ADMISSION='N', INFANT_NICU_ADMISSION_RECODE=0),\n",
       " Row(INFANT_NICU_ADMISSION='U', INFANT_NICU_ADMISSION_RECODE=0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "births.select([\n",
    "        'INFANT_NICU_ADMISSION', \n",
    "        rec_integer(\n",
    "            'INFANT_NICU_ADMISSION', func.lit('YNU')\n",
    "        ).alias('INFANT_NICU_ADMISSION_RECODE') ]\n",
    "     ).take(5)\n",
    "# 增加列有2种方法，一种是基于现在的列计算；\n",
    "#一种是用pyspark.sql.functions的lit()增加常数列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'YNU'>\n",
      "<class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "print (func.lit('YNU'))\n",
    "print (type(func.lit('YNU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform all the `YNU_cols` in one using a list of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs_YNU = [\n",
    "    rec_integer(x, func.lit('YNU')).alias(x) \n",
    "    if x in YNU_cols \n",
    "    else x \n",
    "    for x in births_transformed.columns\n",
    "]\n",
    "\n",
    "births_transformed = births_transformed.select(exprs_YNU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'recode(INFANT_ALIVE_AT_REPORT, YNU) AS `INFANT_ALIVE_AT_REPORT`'> \n",
      " <class 'pyspark.sql.column.Column'> \n",
      "\n",
      "BIRTH_PLACE \n",
      " <class 'str'> \n",
      "\n",
      "MOTHER_AGE_YEARS \n",
      " <class 'str'> \n",
      "\n",
      "FATHER_COMBINED_AGE \n",
      " <class 'str'> \n",
      "\n",
      "CIG_BEFORE \n",
      " <class 'str'> \n",
      "\n",
      "CIG_1_TRI \n",
      " <class 'str'> \n",
      "\n",
      "CIG_2_TRI \n",
      " <class 'str'> \n",
      "\n",
      "CIG_3_TRI \n",
      " <class 'str'> \n",
      "\n",
      "MOTHER_HEIGHT_IN \n",
      " <class 'str'> \n",
      "\n",
      "MOTHER_PRE_WEIGHT \n",
      " <class 'str'> \n",
      "\n",
      "MOTHER_DELIVERY_WEIGHT \n",
      " <class 'str'> \n",
      "\n",
      "MOTHER_WEIGHT_GAIN \n",
      " <class 'str'> \n",
      "\n",
      "Column<b'recode(DIABETES_PRE, YNU) AS `DIABETES_PRE`'> \n",
      " <class 'pyspark.sql.column.Column'> \n",
      "\n",
      "Column<b'recode(DIABETES_GEST, YNU) AS `DIABETES_GEST`'> \n",
      " <class 'pyspark.sql.column.Column'> \n",
      "\n",
      "Column<b'recode(HYP_TENS_PRE, YNU) AS `HYP_TENS_PRE`'> \n",
      " <class 'pyspark.sql.column.Column'> \n",
      "\n",
      "Column<b'recode(HYP_TENS_GEST, YNU) AS `HYP_TENS_GEST`'> \n",
      " <class 'pyspark.sql.column.Column'> \n",
      "\n",
      "Column<b'recode(PREV_BIRTH_PRETERM, YNU) AS `PREV_BIRTH_PRETERM`'> \n",
      " <class 'pyspark.sql.column.Column'> \n",
      "\n",
      "\n",
      " <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for i in exprs_YNU:\n",
    "    print (i,'\\n',type(i),'\\n')\n",
    "print ('\\n',type(exprs_YNU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we got it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+------------+-------------+------------------+\n",
      "|DIABETES_PRE|DIABETES_GEST|HYP_TENS_PRE|HYP_TENS_GEST|PREV_BIRTH_PRETERM|\n",
      "+------------+-------------+------------+-------------+------------------+\n",
      "|           0|            0|           0|            0|                 0|\n",
      "|           0|            0|           0|            0|                 0|\n",
      "|           0|            0|           0|            0|                 0|\n",
      "|           0|            0|           0|            0|                 1|\n",
      "|           0|            0|           0|            0|                 0|\n",
      "+------------+-------------+------------+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "births_transformed.select(YNU_cols[-5:]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get to know your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will use the `colStats(...)` method.\n",
    "\n",
    "为了清晰地建造一个统计模型，需要深入了解数据集的知识。不了解数据便来建造一个成功的模型是有可能的，但是接下来的任务会更艰巨，其需要更多的技术资源来测试所有可能的特征组合。因此在花了80%的时间清理数据后，我们接下来会用15%的时间来了解数据。\n",
    "\n",
    "我通常从描述性统计开始。虽然DataFrame公开了.describe（）方法，但由于我们正在使用MLlib，所以我们将采取.colStats（...）方法。\n",
    "\n",
    "该方法使用RDD的数据来计算MultivariateStatisticalSummary对象的描述性统计信息，并返回MultivariateStatisticalSummary对象，该对象包含如下描述性统计信息：\n",
    "- count（）：行数\n",
    "- max（）：列中的最大值\n",
    "- mean（）：列的所有值的平均值\n",
    "- min（）：列中的最小值\n",
    "- normL1（）：列中值的L1-Norm值\n",
    "- normL2（）：列中值的L2-Norm值\n",
    "- numNonzeros（）：列中非零值的数量\n",
    "- variance（）：列中值的方差值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOTHER_AGE_YEARS: \t28.30 \t 6.08\n",
      "FATHER_COMBINED_AGE: \t44.55 \t 27.55\n",
      "CIG_BEFORE: \t1.43 \t 5.18\n",
      "CIG_1_TRI: \t0.91 \t 3.83\n",
      "CIG_2_TRI: \t0.70 \t 3.31\n",
      "CIG_3_TRI: \t0.58 \t 3.11\n",
      "MOTHER_HEIGHT_IN: \t65.12 \t 6.45\n",
      "MOTHER_PRE_WEIGHT: \t214.50 \t 210.21\n",
      "MOTHER_DELIVERY_WEIGHT: \t223.63 \t 180.01\n",
      "MOTHER_WEIGHT_GAIN: \t30.74 \t 26.23\n"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.stat as st\n",
    "import numpy as np\n",
    "\n",
    "numeric_cols = ['MOTHER_AGE_YEARS','FATHER_COMBINED_AGE',\n",
    "                'CIG_BEFORE','CIG_1_TRI','CIG_2_TRI','CIG_3_TRI',\n",
    "                'MOTHER_HEIGHT_IN','MOTHER_PRE_WEIGHT',\n",
    "                'MOTHER_DELIVERY_WEIGHT','MOTHER_WEIGHT_GAIN'\n",
    "               ]\n",
    "\n",
    "numeric_rdd = births_transformed\\\n",
    "                       .select(numeric_cols)\\\n",
    "                       .rdd \\\n",
    "                       .map(lambda row: [e for e in row])\n",
    "# 直接在RDD上调用map，传递的参数是一整行？\n",
    "\n",
    "mllib_stats = st.Statistics.colStats(numeric_rdd)\n",
    "\n",
    "for col, m, v in zip(numeric_cols, \n",
    "                     mllib_stats.mean(), \n",
    "                     mllib_stats.variance()):\n",
    "    print('{0}: \\t{1:.2f} \\t {2:.2f}'.format(col, m, np.sqrt(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如你所看到的，与父亲相比，母亲年龄更小：母亲的平均年龄是28岁，而父亲的平均年龄则超过44岁。一个好的现象（至少对于一些婴儿来说）是许多母亲怀孕后开始戒烟；而令人吃惊的是，有些母亲还是持续吸烟。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categorical variables we will calculate the frequencies of their values.\n",
    "\n",
    "对于分类变量，我们将计算其值的频率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFANT_ALIVE_AT_REPORT [(1, 23349), (0, 22080)]\n",
      "BIRTH_PLACE [('1', 44558), ('4', 327), ('3', 224), ('2', 136), ('7', 91), ('5', 74), ('6', 11), ('9', 8)]\n",
      "DIABETES_PRE [(0, 44881), (1, 548)]\n",
      "DIABETES_GEST [(0, 43451), (1, 1978)]\n",
      "HYP_TENS_PRE [(0, 44348), (1, 1081)]\n",
      "HYP_TENS_GEST [(0, 43302), (1, 2127)]\n",
      "PREV_BIRTH_PRETERM [(0, 43088), (1, 2341)]\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [e for e in births_transformed.columns \n",
    "                    if e not in numeric_cols]\n",
    "\n",
    "categorical_rdd = births_transformed\\\n",
    "                       .select(categorical_cols)\\\n",
    "                       .rdd \\\n",
    "                       .map(lambda row: [e for e in row])\n",
    "\n",
    "# groupBy算子接收一个函数，这个函数返回的值作为key，\n",
    "# 然后通过这个key来对里面的元素进行分组。\n",
    "# .groupBy(lambda row: row[i]) 应该是使用第i列的数据进行分组           \n",
    "for i, col in enumerate(categorical_cols):\n",
    "    agg = categorical_rdd \\\n",
    "        .groupBy(lambda row: row[i]) \\\n",
    "        .map(lambda row: (row[0], len(row[1])))\n",
    "# 推测，groupBy产生的数据格式就是(key , count_key)\n",
    "\n",
    "    print(col, sorted(agg.collect(), \n",
    "                      key=lambda el: el[1], \n",
    "                      reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------+----------------+-------------------+---------+----------------+-----------------+------------+-------------+------------+-------------+------------------+\n",
      "|INFANT_ALIVE_AT_REPORT|BIRTH_PLACE|MOTHER_AGE_YEARS|FATHER_COMBINED_AGE|CIG_1_TRI|MOTHER_HEIGHT_IN|MOTHER_PRE_WEIGHT|DIABETES_PRE|DIABETES_GEST|HYP_TENS_PRE|HYP_TENS_GEST|PREV_BIRTH_PRETERM|\n",
      "+----------------------+-----------+----------------+-------------------+---------+----------------+-----------------+------------+-------------+------------+-------------+------------------+\n",
      "|                     0|          1|              29|                 99|        0|              99|              999|           0|            0|           0|            0|                 0|\n",
      "|                     0|          1|              22|                 29|        0|              65|              180|           0|            0|           0|            0|                 0|\n",
      "+----------------------+-----------+----------------+-------------------+---------+----------------+-----------------+------------+-------------+------------+-------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "----------------\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, '1', 0, 0, 0, 0, 0],\n",
       " [0, '1', 0, 0, 0, 0, 0],\n",
       " [0, '1', 0, 0, 0, 0, 0],\n",
       " [0, '1', 0, 0, 0, 0, 1],\n",
       " [0, '1', 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "births_transformed.show(2)\n",
    "print ('----------------')\n",
    "print (type(categorical_rdd))\n",
    "categorical_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t INFANT_ALIVE_AT_REPORT\n",
      "PythonRDD[573] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(categorical_cols):\n",
    "    agg = categorical_rdd \\\n",
    "        .groupBy(lambda row: row[i]) \\\n",
    "        .map(lambda row: (row[0], len(row[1])))\n",
    "    print (i,'\\t',col) \n",
    "    print (agg)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD.map()示意\n",
    "![](https://upload-images.jianshu.io/upload_images/12486617-47e5a6fc799795c1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/822/format/webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations between our features.\n",
    "\n",
    "下面的代码将计算相关性矩阵，并且只打印那些相关性系数大于0.5的特征，即corrs>0.5所限制的部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIG_BEFORE-to-CIG_1_TRI: 0.83\n",
      "CIG_BEFORE-to-CIG_2_TRI: 0.72\n",
      "CIG_BEFORE-to-CIG_3_TRI: 0.62\n",
      "CIG_1_TRI-to-CIG_BEFORE: 0.83\n",
      "CIG_1_TRI-to-CIG_2_TRI: 0.87\n",
      "CIG_1_TRI-to-CIG_3_TRI: 0.76\n",
      "CIG_2_TRI-to-CIG_BEFORE: 0.72\n",
      "CIG_2_TRI-to-CIG_1_TRI: 0.87\n",
      "CIG_2_TRI-to-CIG_3_TRI: 0.89\n",
      "CIG_3_TRI-to-CIG_BEFORE: 0.62\n",
      "CIG_3_TRI-to-CIG_1_TRI: 0.76\n",
      "CIG_3_TRI-to-CIG_2_TRI: 0.89\n",
      "MOTHER_PRE_WEIGHT-to-MOTHER_DELIVERY_WEIGHT: 0.54\n",
      "MOTHER_PRE_WEIGHT-to-MOTHER_WEIGHT_GAIN: 0.65\n",
      "MOTHER_DELIVERY_WEIGHT-to-MOTHER_PRE_WEIGHT: 0.54\n",
      "MOTHER_DELIVERY_WEIGHT-to-MOTHER_WEIGHT_GAIN: 0.60\n",
      "MOTHER_WEIGHT_GAIN-to-MOTHER_PRE_WEIGHT: 0.65\n",
      "MOTHER_WEIGHT_GAIN-to-MOTHER_DELIVERY_WEIGHT: 0.60\n"
     ]
    }
   ],
   "source": [
    "corrs = st.Statistics.corr(numeric_rdd)\n",
    "\n",
    "for i, el in enumerate(corrs > 0.5):\n",
    "    correlated = [\n",
    "        (numeric_cols[j], corrs[i][j]) \n",
    "        for j, e in enumerate(el) \n",
    "        if e == 1.0 and j != i]\n",
    "    \n",
    "    if len(correlated) > 0:\n",
    "        for e in correlated:\n",
    "            print('{0}-to-{1}: {2:.2f}' \\\n",
    "                  .format(numeric_cols[i], e[0], e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00 -3.52028046e-02 -6.41010571e-02 -4.52544240e-02\n",
      "  -3.35687670e-02 -2.73199103e-02  4.19113600e-02  2.85203229e-02\n",
      "   2.23328210e-02  1.46244230e-02]\n",
      " [-3.52028046e-02  1.00000000e+00  8.79929846e-02  9.43624397e-02\n",
      "   9.14375554e-02  7.61406630e-02  8.62027558e-02  1.27899779e-01\n",
      "   9.75061365e-02  3.54820308e-02]\n",
      " [-6.41010571e-02  8.79929846e-02  1.00000000e+00  8.25531201e-01\n",
      "   7.22135074e-01  6.23033578e-01 -1.08714989e-02 -2.64243555e-02\n",
      "  -4.67241960e-03 -1.18809418e-02]\n",
      " [-4.52544240e-02  9.43624397e-02  8.25531201e-01  1.00000000e+00\n",
      "   8.65457361e-01  7.59919819e-01 -6.38108725e-03 -1.23276110e-02\n",
      "  -1.12750503e-03 -1.48184443e-02]\n",
      " [-3.35687670e-02  9.14375554e-02  7.22135074e-01  8.65457361e-01\n",
      "   1.00000000e+00  8.93075782e-01 -2.76545239e-03 -6.06184326e-03\n",
      "   1.46010680e-03 -1.43591588e-02]\n",
      " [-2.73199103e-02  7.61406630e-02  6.23033578e-01  7.59919819e-01\n",
      "   8.93075782e-01  1.00000000e+00 -9.37520764e-04 -3.77646699e-03\n",
      "   4.83568294e-03 -6.37879421e-03]\n",
      " [ 4.19113600e-02  8.62027558e-02 -1.08714989e-02 -6.38108725e-03\n",
      "  -2.76545239e-03 -9.37520764e-04  1.00000000e+00  4.52569866e-01\n",
      "   4.74216957e-01  3.31764226e-01]\n",
      " [ 2.85203229e-02  1.27899779e-01 -2.64243555e-02 -1.23276110e-02\n",
      "  -6.06184326e-03 -3.77646699e-03  4.52569866e-01  1.00000000e+00\n",
      "   5.35970067e-01  6.49941338e-01]\n",
      " [ 2.23328210e-02  9.75061365e-02 -4.67241960e-03 -1.12750503e-03\n",
      "   1.46010680e-03  4.83568294e-03  4.74216957e-01  5.35970067e-01\n",
      "   1.00000000e+00  5.96928500e-01]\n",
      " [ 1.46244230e-02  3.54820308e-02 -1.18809418e-02 -1.48184443e-02\n",
      "  -1.43591588e-02 -6.37879421e-03  3.31764226e-01  6.49941338e-01\n",
      "   5.96928500e-01  1.00000000e+00]]\n",
      "0 \t [ True False False False False False False False False False]\n",
      "1 \t [False  True False False False False False False False False]\n",
      "2 \t [False False  True  True  True  True False False False False]\n",
      "3 \t [False False  True  True  True  True False False False False]\n",
      "4 \t [False False  True  True  True  True False False False False]\n",
      "5 \t [False False  True  True  True  True False False False False]\n",
      "6 \t [False False False False False False  True False False False]\n",
      "7 \t [False False False False False False False  True  True  True]\n",
      "8 \t [False False False False False False False  True  True  True]\n",
      "9 \t [False False False False False False False  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print (corrs)\n",
    "for x,y in enumerate(corrs > 0.5):\n",
    "    print (x,'\\t',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can drop most of highly correlated features. \n",
    "\n",
    "如你所见，“CIG_...”特征是高度相关的，所以它们中的大部分我们用不到。由于我们要尽快预测婴儿的生存机会，所以我们只会保留“CIG_1_TRI”。另外，如预期的那样，重量特征也是高度相关的，我们只会保留“MOTHER_PRE_WEIGHT”："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = [\n",
    "    'INFANT_ALIVE_AT_REPORT', \n",
    "    'BIRTH_PLACE', \n",
    "    'MOTHER_AGE_YEARS', \n",
    "    'FATHER_COMBINED_AGE', \n",
    "    'CIG_1_TRI', \n",
    "    'MOTHER_HEIGHT_IN', \n",
    "    'MOTHER_PRE_WEIGHT', \n",
    "    'DIABETES_PRE', \n",
    "    'DIABETES_GEST', \n",
    "    'HYP_TENS_PRE', \n",
    "    'HYP_TENS_GEST', \n",
    "    'PREV_BIRTH_PRETERM'\n",
    "]\n",
    "births_transformed = births_transformed.select([e for e in features_to_keep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a Chi-square test to determine if there are significant differences for categorical variables.\n",
    "\n",
    "我们无法计算分类特征的相关性。然而，我们可以进行卡方检验来确定是否存在显著差异。\n",
    "以下是如何使用MLlib的.chiSqTest（...）方法：\n",
    "\n",
    "卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，如果卡方值越大，二者偏差程度越大；反之，二者偏差越小；若两个值完全相等时，卡方值就为0，表明理论值完全符合。\n",
    "\n",
    "我们循环遍历所有的分类变量，并通过“INFANT_ALIVE_AT_REPORT”特征进行转换，以获得计数。接下来，我们将它们转换为RDD，因此我们可以使用pyspark.mllib.linalg模块将它们转换成矩阵。\n",
    ".Matrices.dense（...）方法的第一个参数指定矩阵的行数，在我们的例子中，这是分类特征的不同值的长度。\n",
    "\n",
    "第二个参数指定列数：因为我们的“INFANT_ALIVE_AT_REPORT”目标变量只有两个值，所以列数为二。\n",
    "最后一个参数是要转换为矩阵的值的列表。\n",
    "这是一个更清楚的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[1., 4.],\n",
      "             [2., 5.],\n",
      "             [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print (ln.Matrices.dense(3,2,[1,2,3,4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIRTH_PLACE 0.0\n",
      "DIABETES_PRE 0.0\n",
      "DIABETES_GEST 0.0\n",
      "HYP_TENS_PRE 0.0\n",
      "HYP_TENS_GEST 0.0\n",
      "PREV_BIRTH_PRETERM 0.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.linalg as ln\n",
    "\n",
    "for cat in categorical_cols[1:]:\n",
    "    agg = births_transformed \\\n",
    "        .groupby('INFANT_ALIVE_AT_REPORT') \\\n",
    "        .pivot(cat) \\\n",
    "        .count()    \n",
    "    \n",
    "    agg_rdd = agg \\\n",
    "        .rdd\\\n",
    "        .map(lambda row: (row[1:])) \\\n",
    "        .flatMap(lambda row: \n",
    "                 [0 if e == None else e for e in row]) \\\n",
    "        .collect()\n",
    "\n",
    "    row_length = len(agg.collect()[0]) - 1\n",
    "    agg_dense = ln.Matrices.dense(row_length, 2, agg_rdd)\n",
    "    \n",
    "    test = st.Statistics.chiSqTest(agg_dense)\n",
    "    print(cat, round(test.pValue, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的测试表明，所有的特征应该是显著不同的，应该有助于我们预测婴儿的生存机会"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIRTH_PLACE\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+----------------------+-----+---+---+---+---+---+---+----+\n",
      "|INFANT_ALIVE_AT_REPORT|    1|  2|  3|  4|  5|  6|  7|   9|\n",
      "+----------------------+-----+---+---+---+---+---+---+----+\n",
      "|                     1|22995|113|158| 39| 19|  2| 23|null|\n",
      "|                     0|21563| 23| 66|288| 55|  9| 68|   8|\n",
      "+----------------------+-----+---+---+---+---+---+---+----+\n",
      "\n",
      "<class 'list'>\n",
      "[22995, 113, 158, 39, 19, 2, 23, 0, 21563, 23, 66, 288, 55, 9, 68, 8]\n",
      "<class 'pyspark.mllib.linalg.DenseMatrix'>\n",
      "DIABETES_PRE\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+----------------------+-----+---+\n",
      "|INFANT_ALIVE_AT_REPORT|    0|  1|\n",
      "+----------------------+-----+---+\n",
      "|                     1|23178|171|\n",
      "|                     0|21703|377|\n",
      "+----------------------+-----+---+\n",
      "\n",
      "<class 'list'>\n",
      "[23178, 171, 21703, 377]\n",
      "<class 'pyspark.mllib.linalg.DenseMatrix'>\n"
     ]
    }
   ],
   "source": [
    "# 测试两个\n",
    "for cat in categorical_cols[1:3]:\n",
    "    agg = births_transformed \\\n",
    "        .groupby('INFANT_ALIVE_AT_REPORT') \\\n",
    "        .pivot(cat) \\\n",
    "        .count()    \n",
    "# pivot 行转列，结合groupby语句，就是针对cat中的每一个不同的值，新建列，列名为值，\n",
    "# 列中的value是count\n",
    "\n",
    "    print (cat)\n",
    "    print(type(agg))\n",
    "    agg.show()\n",
    "    \n",
    "    # agg_rdd是list，把agg中的第一列以外的数据抽出来，flatmap为list\n",
    "    agg_rdd = agg \\\n",
    "        .rdd\\\n",
    "        .map(lambda row: (row[1:])) \\\n",
    "        .flatMap(lambda row: \n",
    "                 [0 if e == None else e for e in row]) \\\n",
    "        .collect()\n",
    "\n",
    "    print(type(agg_rdd))\n",
    "    print (agg_rdd)\n",
    "    \n",
    "    row_length = len(agg.collect()[0]) - 1\n",
    "    agg_dense = ln.Matrices.dense(row_length, 2, agg_rdd)\n",
    "    # dense 就是把下面的两行的数据进行转置操作，变成两列的数据。进行卡方检验。\n",
    "    print(type(agg_dense))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an RDD of `LabeledPoint`s\n",
    "\n",
    "We will use a hashing trick to encode the `'BIRTH_PLACE'` feature.\n",
    "\n",
    "因此，现在可以创建用于构建我们的模型的最终数据集了。我们要把DataFrame转换为LabeledPoint的RDD。\n",
    "\n",
    "LabeledPoint是一种MLlib的数据结构，用于训练机器学习模型。它由两个属性组成：标签和特征。\n",
    "标签是我们的目标变量，特征可以是NumPy数组、列表、pyspark.mllib.linalg.SparseVector、pyspark.mllib.linalg.DenseVector或scipy.sparse列矩阵。\n",
    "\n",
    "在构建最终数据集之前，首先需要解决一个最后的障碍：“BIRTH_PLACE”特征仍然是一个字符串。其他分类变量都可以直接使用（因为它们现在是虚拟变量），故我们将使用散列技巧来编码“BIRTH_PLACE”特征：\n",
    "\n",
    "首先，我们创建哈希模型。因为我们的特征有七个级别，所以我们使用与哈希处理中相同多的特征。接下来，真正要做的是使用该模型将“BIRTH_PLACE”特征转换为SparseVector。如果您的数据集有许多列，但是在一行中只有少数数据具有非零值，则这种数据结构是首选的。然后，我们将所有特征结合在一起，最终创建一个LabeledPoint。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.mllib.feature as ft\n",
    "import pyspark.mllib.regression as reg\n",
    "\n",
    "hashing = ft.HashingTF(7)\n",
    "# 为什么是7？BIRTH_PLACE 有8个不同的值\n",
    "\n",
    "births_hashed = births_transformed \\\n",
    "    .rdd \\\n",
    "    .map(lambda row: [ # row 是行\n",
    "            list(hashing.transform(row[1]).toArray()) # 因为 BIRTH_PLACE 是第一列\n",
    "                if col == 'BIRTH_PLACE' \n",
    "                else row[i] \n",
    "            for i, col \n",
    "            in enumerate(features_to_keep)]) \\\n",
    "    .map(lambda row: [[e] if type(e) == int else e \n",
    "                      for e in row]) \\\n",
    "    .map(lambda row: [item for sublist in row   # ？？？\n",
    "                      for item in sublist]) \\\n",
    "    .map(lambda row: reg.LabeledPoint(\n",
    "            row[0], \n",
    "            ln.Vectors.dense(row[1:]))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before we move to the modeling stage, we need to split our dataset into two sets: one we'll use for training and another one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "births_train, births_test = births_hashed.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "(0.0,[1.0,0.0,0.0,0.0,0.0,0.0,0.0,29.0,99.0,0.0,99.0,999.0,0.0,0.0,0.0,0.0,0.0])\n",
      "(0.0,[1.0,0.0,0.0,0.0,0.0,0.0,0.0,22.0,29.0,0.0,65.0,180.0,0.0,0.0,0.0,0.0,0.0])\n",
      "(0.0,[1.0,0.0,0.0,0.0,0.0,0.0,0.0,38.0,40.0,0.0,63.0,155.0,0.0,0.0,0.0,0.0,0.0])\n",
      "(0.0,[1.0,0.0,0.0,0.0,0.0,0.0,0.0,18.0,99.0,4.0,61.0,110.0,0.0,0.0,0.0,0.0,0.0])\n",
      "(0.0,[1.0,0.0,0.0,0.0,0.0,0.0,0.0,25.0,26.0,0.0,64.0,136.0,0.0,0.0,0.0,0.0,0.0])\n"
     ]
    }
   ],
   "source": [
    "print (type(births_train))\n",
    "for i in births_train.collect()[:5]:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting infant survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Logistic regression in Spark\n",
    "\n",
    "MLLib used to provide a logistic regression model estimated using a stochastic gradient descent (SGD) algorithm. This model has been deprecated in Spark 2.0 in favor of the `LogisticRegressionWithLBFGS` model. \n",
    "\n",
    "我们终于可以开始预测婴儿的生存机会了。在本节中，我们将构建两个模型：线性分类器（linear classifier）——逻辑回归，和非线性分类器（non-linear classifier）——随机森林。对于前者，我们使用所有特征来做处理，而对于后者，我们使用ChiSqSelector（...）方法选出前四个特征。\n",
    "\n",
    "逻辑回归从某种程度上说，是构建任何分类模型的基准。MLlib过去使用随机梯度下降（SGD）算法来提供逻辑回归模型的评估。这个模型已经在Spark 2.0中被弃用，而使用LogisticRegressionWithLBFGS模型。\n",
    "\n",
    "LogisticRegressionWithLBFGS模型使用Limited-memoryBroyden-Fletcher-Goldfarb-Shanno（BFGS）优化算法。这是一种接近于BFGS算法的拟牛顿方法。\n",
    "\n",
    "训练模型非常简单：我们只需调用.train（...）方法。需要的参数是带有LabeledPoint的RDD；我们还指定了迭代次数，因此运行时间不会太长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification \\\n",
    "    import LogisticRegressionWithLBFGS\n",
    "\n",
    "LR_Model = LogisticRegressionWithLBFGS \\\n",
    "    .train(births_train, iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use the model to predict the classes for our testing set.\n",
    "\n",
    "下面的代码段创建了一个RDD，其中每个元素都是一个元组，第一个元素是实际的标签，第二个元素是模型的预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_results = (\n",
    "        births_test.map(lambda row: row.label) \\\n",
    "        .zip(LR_Model.predict(births_test.map(lambda row: row.features)))\n",
    "    ).map(lambda row: (row[0], row[1] * 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how well or how bad our model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR: 0.80\n",
      "Area under ROC: 0.63\n"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.evaluation as ev\n",
    "LR_evaluation = ev.BinaryClassificationMetrics(LR_results)\n",
    "\n",
    "print('Area under PR: {0:.2f}' \\\n",
    "      .format(LR_evaluation.areaUnderPR))\n",
    "print('Area under ROC: {0:.2f}' \\\n",
    "      .format(LR_evaluation.areaUnderROC))\n",
    "LR_evaluation.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型表现相当不错！Precision-Recall曲线下85%的面积指示契合得很好。在这种情况下，我们可能会预测死亡人数略有增加（真正和假正）。而这种情况实际上是一件好事，因为这样可以让医生对怀孕的母亲和婴儿做特别的关注。\n",
    "\n",
    "受试者工作特性（Receiver-Operating Characteristic）（ROC）曲线下的面积可以理解为：与随机选择的负实例相比，模型排名几率高于随机选择的正实例。63%这个值是可以接受的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting only the most predictable features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "MLLib allows us to select the most predictable features using a Chi-Square selector.\n",
    "\n",
    "任何使用较少特征来准确预测类的模型，应始终优于使用复杂特征的模型。MLlib允许使用Chi-Square选择器来选择最可预测的特征。\n",
    "\n",
    "我们要求选择器从数据集返回四个最具预测性的特征，并使用births_train数据集训练选择器。然后，我们使用该模型从训练和测试的数据集中仅仅提取这些特征。\n",
    "\n",
    ".ChiSqSelector（...）方法只能用于数字特征；在使用选择器之前，分类变量需要进行散列或伪编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ft.ChiSqSelector(4).fit(births_train)\n",
    "\n",
    "topFeatures_train = (\n",
    "        births_train.map(lambda row: row.label) \\\n",
    "        .zip(selector.transform(births_train.map(lambda row: row.features)))\n",
    "    ).map(lambda row: reg.LabeledPoint(row[0], row[1]))\n",
    "\n",
    "topFeatures_test = (\n",
    "        births_test.map(lambda row: row.label) \\\n",
    "        .zip(selector.transform(births_test.map(lambda row: row.features)))\n",
    "    ).map(lambda row: reg.LabeledPoint(row[0], row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "(0.0,[0.0,42.0,60.0,128.0])\n",
      "(0.0,[0.0,37.0,66.0,150.0])\n",
      "(0.0,[0.0,25.0,68.0,155.0])\n",
      "(0.0,[0.0,66.0,65.0,140.0])\n",
      "(0.0,[0.0,41.0,59.0,106.0])\n"
     ]
    }
   ],
   "source": [
    "print (type(topFeatures_test))\n",
    "for i in topFeatures_test.collect()[:5]:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest in Spark\n",
    "\n",
    "We are now ready to build the random forest model. \n",
    "\n",
    ".trainClassifier（...）方法的第一个参数指定训练数据集。参数numClasses表示我们的目标变量有多少类。作为第三个参数，您可以传递一个dictionary，其中键是RDD中分类特征的索引，键值表示分类特征具有的级别数。numTrees指定森林中树的数目。下一个参数告诉模型使用数据集中的所有特征，而不是只保留最具描述性的特征，而最后一个参数指定模型随机部分的种子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "RF_model = RandomForest \\\n",
    "    .trainClassifier(data=topFeatures_train, \n",
    "                     numClasses=2, \n",
    "                     categoricalFeaturesInfo={}, \n",
    "                     numTrees=6,  \n",
    "                     featureSubsetStrategy='all',\n",
    "                     seed=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well our model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR: 0.76\n",
      "Area under ROC: 0.62\n"
     ]
    }
   ],
   "source": [
    "RF_results = (\n",
    "        topFeatures_test.map(lambda row: row.label) \\\n",
    "        .zip(RF_model.predict(topFeatures_test.map(lambda row: row.features)))\n",
    "    )\n",
    "\n",
    "RF_evaluation = ev.BinaryClassificationMetrics(RF_results)\n",
    "\n",
    "print('Area under PR: {0:.2f}' \\\n",
    "      .format(RF_evaluation.areaUnderPR))\n",
    "print('Area under ROC: {0:.2f}' \\\n",
    "      .format(RF_evaluation.areaUnderROC))\n",
    "RF_evaluation.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the logistic regression would perform with reduced number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR: 0.83\n",
      "Area under ROC: 0.63\n"
     ]
    }
   ],
   "source": [
    "LR_Model_2 = LogisticRegressionWithLBFGS \\\n",
    "    .train(topFeatures_train, iterations=10)\n",
    "\n",
    "LR_results_2 = (\n",
    "        topFeatures_test.map(lambda row: row.label) \\\n",
    "        .zip(LR_Model_2 \\\n",
    "             .predict(topFeatures_test \\\n",
    "                      .map(lambda row: row.features)))\n",
    "    ).map(lambda row: (row[0], row[1] * 1.0))\n",
    "\n",
    "LR_evaluation_2 = ev.BinaryClassificationMetrics(LR_results_2)\n",
    "\n",
    "print('Area under PR: {0:.2f}' \\\n",
    "      .format(LR_evaluation_2.areaUnderPR))\n",
    "print('Area under ROC: {0:.2f}' \\\n",
    "      .format(LR_evaluation_2.areaUnderROC))\n",
    "LR_evaluation_2.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本章中，我们浏览了PySpark的MLlib软件包的功能。虽然该软件包目前处于维护模式，并且没有被积极地推进，但了解如何使用仍然是有益处的。而且它是目前唯一提供用流数据训练模型的软件包。我们使用了MLlib来清理、转换和熟悉婴儿死亡数据集。使用这些知识，我们成功地建立了两个模型，旨在根据其母亲、父亲和出生地点的信息来预测婴儿生存的机会。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter Python 3.7.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
